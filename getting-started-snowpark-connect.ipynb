{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80e17fae",
   "metadata": {},
   "source": [
    "# Getting Started with Snowpark Connect for Apache Spark\n",
    "\n",
    "Welcome to Snowpark Connect for Apache Spark!\n",
    "\n",
    "This notebook demonstrates how to run familiar PySpark workloads directly on Snowflake\n",
    "using the Tasty Bytes food truck dataset. You'll learn:\n",
    "- Environment setup\n",
    "- PySpark DataFrame API fundamentals\n",
    "- Data exploration and transformation capabilities\n",
    "- Performance benefits and limitations\n",
    "- Best practices for Spark-on-Snowflake workflows\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab7794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup - TODO REPLACE WITH ACTUAL IMPORTS\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize Spark session for Snowflake\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TastyBytes-PySpark-Demo\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Connection parameters (pre-configured in learning environment)\n",
    "print(\"‚úÖ Spark session initialized successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d022cb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tasty Bytes core tables - TODO - UPDATE WITH ACTUAL TABLES\n",
    "order_header = spark.table(\"FROSTBYTE_TASTY_BYTES.RAW_POS.ORDER_HEADER\")\n",
    "order_detail = spark.table(\"FROSTBYTE_TASTY_BYTES.RAW_POS.ORDER_DETAIL\") \n",
    "menu = spark.table(\"FROSTBYTE_TASTY_BYTES.RAW_POS.MENU\")\n",
    "truck = spark.table(\"FROSTBYTE_TASTY_BYTES.RAW_POS.TRUCK\")\n",
    "location = spark.table(\"FROSTBYTE_TASTY_BYTES.RAW_POS.LOCATION\")\n",
    "\n",
    "# Schema exploration - PySpark's power in data discovery\n",
    "print(\"üìä ORDER_HEADER Schema:\")\n",
    "order_header.printSchema()\n",
    "\n",
    "print(\"\\nüìà Dataset sizes:\")\n",
    "tables = [\n",
    "    (\"ORDER_HEADER\", order_header),\n",
    "    (\"ORDER_DETAIL\", order_detail),\n",
    "    (\"MENU\", menu),\n",
    "    (\"TRUCK\", truck),\n",
    "    (\"LOCATION\", location)\n",
    "]\n",
    "\n",
    "for name, df in tables:\n",
    "    count = df.count()\n",
    "    print(f\"{name}: {count:,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1914d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO update with appropriate APIs and string literals\n",
    "# SELECT operations - Column selection and aliasing\n",
    "sales_summary = order_header.select(\n",
    "    col(\"ORDER_ID\"),\n",
    "    col(\"ORDER_TS\").alias(\"order_timestamp\"),\n",
    "    col(\"ORDER_AMOUNT\").alias(\"total_amount\"),\n",
    "    col(\"TRUCK_ID\"),\n",
    "    col(\"LOCATION_ID\")\n",
    ")\n",
    "\n",
    "# FILTER operations - Data filtering with multiple conditions\n",
    "recent_orders = sales_summary.filter(\n",
    "    (col(\"order_timestamp\") >= \"2022-01-01\") & \n",
    "    (col(\"total_amount\") > 10.0)\n",
    ")\n",
    "\n",
    "# AGGREGATION operations - GroupBy and statistical functions\n",
    "daily_sales = order_header \\\n",
    "    .withColumn(\"order_date\", to_date(col(\"ORDER_TS\"))) \\\n",
    "    .groupBy(\"order_date\") \\\n",
    "    .agg(\n",
    "        count(\"ORDER_ID\").alias(\"total_orders\"),\n",
    "        sum(\"ORDER_AMOUNT\").alias(\"daily_revenue\"),\n",
    "        avg(\"ORDER_AMOUNT\").alias(\"avg_order_value\"),\n",
    "        max(\"ORDER_AMOUNT\").alias(\"max_order\"),\n",
    "        min(\"ORDER_AMOUNT\").alias(\"min_order\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"order_date\"))\n",
    "\n",
    "print(\"üìÖ Daily Sales Performance:\")\n",
    "daily_sales.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191a359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - add actual APIs\n",
    "# Advanced Spark Transformations\n",
    "# WINDOW FUNCTIONS - Advanced analytics capabilities\n",
    "window_spec = Window.partitionBy(\"TRUCK_ID\").orderBy(\"ORDER_TS\")\n",
    "\n",
    "truck_performance = order_header \\\n",
    "    .withColumn(\"running_total\", sum(\"ORDER_AMOUNT\").over(window_spec)) \\\n",
    "    .withColumn(\"order_rank\", row_number().over(window_spec)) \\\n",
    "    .withColumn(\"prev_order_amount\", lag(\"ORDER_AMOUNT\", 1).over(window_spec))\n",
    "\n",
    "# JOINS - Combining multiple datasets\n",
    "enriched_orders = order_header \\\n",
    "    .join(truck, \"TRUCK_ID\") \\\n",
    "    .join(location, \"LOCATION_ID\") \\\n",
    "    .select(\n",
    "        col(\"ORDER_ID\"),\n",
    "        col(\"ORDER_TS\"),\n",
    "        col(\"ORDER_AMOUNT\"),\n",
    "        col(\"TRUCK_BRAND_NAME\"),\n",
    "        col(\"PRIMARY_CITY\"),\n",
    "        col(\"REGION\"),\n",
    "        col(\"COUNTRY\")\n",
    "    )\n",
    "\n",
    "print(\"üöö Enriched Orders with Truck and Location Data:\")\n",
    "enriched_orders.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9ea376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex Analytics - Menu Item Performance\n",
    "# TODO - update with actual APIs and string literals\n",
    "# Complex multi-table analysis\n",
    "menu_performance = order_detail \\\n",
    "    .join(order_header, \"ORDER_ID\") \\\n",
    "    .join(menu, \"MENU_ITEM_ID\") \\\n",
    "    .groupBy(\"MENU_ITEM_NAME\", \"ITEM_CATEGORY\") \\\n",
    "    .agg(\n",
    "        count(\"ORDER_DETAIL_ID\").alias(\"times_ordered\"),\n",
    "        sum(\"QUANTITY\").alias(\"total_quantity\"),\n",
    "        sum(\"PRICE\").alias(\"total_revenue\"),\n",
    "        avg(\"PRICE\").alias(\"avg_price\")\n",
    "    ) \\\n",
    "    .withColumn(\"revenue_per_order\", col(\"total_revenue\") / col(\"times_ordered\")) \\\n",
    "    .orderBy(desc(\"total_revenue\"))\n",
    "\n",
    "print(\"üçî Top Menu Items by Revenue:\")\n",
    "menu_performance.show(15)\n",
    "\n",
    "# Time-based analysis with date functions\n",
    "seasonal_trends = order_header \\\n",
    "    .withColumn(\"month\", month(col(\"ORDER_TS\"))) \\\n",
    "    .withColumn(\"day_of_week\", dayofweek(col(\"ORDER_TS\"))) \\\n",
    "    .withColumn(\"hour\", hour(col(\"ORDER_TS\"))) \\\n",
    "    .groupBy(\"month\", \"day_of_week\") \\\n",
    "    .agg(\n",
    "        count(\"ORDER_ID\").alias(\"order_count\"),\n",
    "        avg(\"ORDER_AMOUNT\").alias(\"avg_amount\")\n",
    "    ) \\\n",
    "    .orderBy(\"month\", \"day_of_week\")\n",
    "\n",
    "print(\"üìà Seasonal Ordering Patterns:\")\n",
    "seasonal_trends.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5aca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark SQL Integration\n",
    "# TODO - verfiy sql and apis\n",
    "# Register DataFrames as temporary views for SQL access\n",
    "order_header.createOrReplaceTempView(\"orders\")\n",
    "order_detail.createOrReplaceTempView(\"order_items\")\n",
    "menu.createOrReplaceTempView(\"menu_items\")\n",
    "truck.createOrReplaceTempView(\"trucks\")\n",
    "\n",
    "# Complex SQL queries using Spark SQL\n",
    "top_performing_trucks = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        t.TRUCK_BRAND_NAME,\n",
    "        t.TRUCK_ID,\n",
    "        COUNT(o.ORDER_ID) as total_orders,\n",
    "        SUM(o.ORDER_AMOUNT) as total_revenue,\n",
    "        AVG(o.ORDER_AMOUNT) as avg_order_value,\n",
    "        RANK() OVER (ORDER BY SUM(o.ORDER_AMOUNT) DESC) as revenue_rank\n",
    "    FROM orders o\n",
    "    JOIN trucks t ON o.TRUCK_ID = t.TRUCK_ID\n",
    "    WHERE o.ORDER_TS >= '2022-01-01'\n",
    "    GROUP BY t.TRUCK_BRAND_NAME, t.TRUCK_ID\n",
    "    ORDER BY total_revenue DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"üèÜ Top Performing Food Trucks:\")\n",
    "top_performing_trucks.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3adefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Optimization Techniques\n",
    "# TODO - validate with eng and product\n",
    "# Caching for iterative analysis\n",
    "daily_sales.cache()\n",
    "print(\"üíæ Cached daily_sales DataFrame for reuse\")\n",
    "\n",
    "# Partitioning awareness\n",
    "partitioned_orders = order_header \\\n",
    "    .withColumn(\"year_month\", date_format(col(\"ORDER_TS\"), \"yyyy-MM\")) \\\n",
    "    .repartition(col(\"year_month\"))\n",
    "\n",
    "# Broadcast joins for small dimension tables\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Efficient join with small lookup table\n",
    "efficient_join = order_header \\\n",
    "    .join(broadcast(truck), \"TRUCK_ID\") \\\n",
    "    .select(\"ORDER_ID\", \"ORDER_AMOUNT\", \"TRUCK_BRAND_NAME\")\n",
    "\n",
    "print(\"‚ö° Applied performance optimizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c369fd1d",
   "metadata": {},
   "source": [
    "## Limitations and Workarounds\n",
    "\n",
    "1. **Unsupported Data Types:**\n",
    "   - DayTimeIntervalType\n",
    "   - YearMonthIntervalType  \n",
    "   - UserDefinedTypes\n",
    "\n",
    "2. **Performance Considerations:**\n",
    "   - Some operations may not push down to Snowflake engine\n",
    "   - Complex UDFs might impact performance\n",
    "   - Large shuffles can be expensive\n",
    "\n",
    "3. **Feature Gaps:**\n",
    "   - Some advanced Spark features may not be available\n",
    "   - Third-party library limitations\n",
    "   - Streaming operations not supported\n",
    "\n",
    "4. **Best Practices:**\n",
    "   - Prefer SQL operations when possible\n",
    "   - Use broadcast joins for small tables\n",
    "   - Cache frequently accessed DataFrames\n",
    "   - Minimize data movement between operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2f758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of operation that might not optimize well\n",
    "try:\n",
    "    # Complex Python UDF (may not push down efficiently)\n",
    "    from pyspark.sql.functions import udf\n",
    "    \n",
    "    @udf(returnType=StringType())\n",
    "    def complex_categorization(amount):\n",
    "        if amount < 10:\n",
    "            return \"Small\"\n",
    "        elif amount < 25:\n",
    "            return \"Medium\"\n",
    "        else:\n",
    "            return \"Large\"\n",
    "    \n",
    "    categorized_orders = order_header \\\n",
    "        .withColumn(\"order_size\", complex_categorization(col(\"ORDER_AMOUNT\")))\n",
    "    \n",
    "    print(\"‚úÖ UDF applied successfully (but may impact performance)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå UDF limitation encountered: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5edcc01",
   "metadata": {},
   "source": [
    "## Migration from Traditional Spark to Snowpark Connect\"\n",
    "\n",
    "**Code Changes Required:**\n",
    "- Minimal changes to existing PySpark code\n",
    "- Update connection configuration\n",
    "- Review UDF implementations\n",
    "- Test performance characteristics\n",
    "\n",
    "**Benefits:**\n",
    "‚úÖ No Spark cluster management\n",
    "‚úÖ Snowflake's security and governance\n",
    "‚úÖ Elastic scaling\n",
    "‚úÖ Integrated with Snowflake ecosystem\n",
    "\n",
    "**Migration Checklist:**\n",
    "‚ñ° Inventory existing Spark jobs\n",
    "‚ñ° Identify unsupported features\n",
    "‚ñ° Test performance on representative workloads\n",
    "‚ñ° Update deployment processes\n",
    "‚ñ° Train team on new architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c236d529",
   "metadata": {},
   "source": [
    "## üéØ Next Steps for Learning\n",
    "\n",
    "1. **Explore Advanced Features:**\n",
    "   - Machine Learning with MLlib\n",
    "   - Streaming data processing\n",
    "   - Graph analytics\n",
    "\n",
    "2. **Performance Tuning:**\n",
    "   - Query optimization techniques\n",
    "   - Partitioning strategies\n",
    "   - Caching best practices\n",
    "\n",
    "3. **Integration Patterns:**\n",
    "   - Snowflake native features\n",
    "   - External data sources\n",
    "   - Orchestration with Airflow\n",
    "\n",
    "4. **Production Deployment:**\n",
    "   - CI/CD pipelines\n",
    "   - Monitoring and alerting\n",
    "   - Error handling strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f823749",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "spark.catalog.clearCache()\n",
    "print(\"üßπ Cleaned up cached data\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
