{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80e17fae",
   "metadata": {},
   "source": [
    "# Guide to Snowpark Connect for Apache Spark\n",
    "\n",
    "This comprehensive guide covers everything you need to know about Snowpark Connect for Apache Spark - from fundamentals to advanced patterns.\n",
    "\n",
    "## What You'll Learn:\n",
    "- What is Snowpark Connect and how it works\n",
    "- Quick start and initialization\n",
    "- Data I/O and transformation patterns\n",
    "- Performance considerations and best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab7794e",
   "metadata": {},
   "source": [
    "## What is Snowpark Connect?\n",
    "\n",
    "Snowpark Connect allows you to run the **PySpark DataFrame API** on **Snowflake infrastructure** - no Spark cluster needed!\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "**Execution Model:**\n",
    "- Your DataFrame operations are translated to Snowflake SQL\n",
    "- Computation happens in Snowflake warehouses (not Spark executors)\n",
    "- Results stream back via Apache Arrow format\n",
    "- No Spark cluster, driver, or executors required\n",
    "\n",
    "**Query Pushdown:**\n",
    "- ‚úÖ **Fully Optimized:** DataFrame operations, SQL functions, aggregations push down to Snowflake\n",
    "- ‚ö†Ô∏è **Performance Impact:** Python UDFs run client-side (fetch data ‚Üí process ‚Üí send back)\n",
    "- üí° **Better Alternative:** Use built-in SQL functions instead of UDFs\n",
    "\n",
    "### Feature Support Matrix:\n",
    "\n",
    "Understanding what PySpark features are supported helps you write efficient code.\n",
    "\n",
    "#### ‚úÖ Fully Supported DataFrame Operations:\n",
    "- `select`, `filter`, `where`\n",
    "- `groupBy`, `agg` (all aggregation functions)\n",
    "- `join` (inner, left, right, outer, broadcast)\n",
    "- `orderBy`, `sort`\n",
    "- `distinct`, `dropDuplicates`\n",
    "- Window functions (`row_number`, `rank`, `lag`, `lead`, etc.)\n",
    "- Built-in functions (95%+ coverage)\n",
    "- `cache`, `persist` (creates temp tables in Snowflake)\n",
    "\n",
    "#### ‚ö†Ô∏è Limited Support:\n",
    "- `repartition` (logical operation only)\n",
    "- `coalesce` (similar to repartition)\n",
    "- Python UDFs (work but slow - avoid if possible)\n",
    "- Pandas UDFs (work but slow - avoid if possible)\n",
    "- MLlib (partial - transformers work, estimators limited)\n",
    "\n",
    "#### ‚ùå NOT Supported:\n",
    "- RDD API completely\n",
    "- `.rdd`, `.foreach()`, `.foreachPartition()`\n",
    "- Structured Streaming\n",
    "- GraphX\n",
    "- Custom data sources\n",
    "- `.checkpoint()`\n",
    "\n",
    "### Data Types Support\n",
    "\n",
    "**‚úÖ Supported:**\n",
    "- String, Integer, Long, Float, Double, Decimal\n",
    "- Boolean, Date, Timestamp\n",
    "- Array, Map, Struct\n",
    "- Binary\n",
    "\n",
    "**‚ùå Not Supported:**\n",
    "- DayTimeIntervalType\n",
    "- YearMonthIntervalType\n",
    "- UserDefinedTypes\n",
    "\n",
    "#### Supported File Formats:\n",
    "- ‚úÖ Parquet, CSV, JSON, Avro, ORC\n",
    "- ‚ùå Delta Lake, Hudi not supported"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d022cb5b",
   "metadata": {},
   "source": [
    "## Quick Start\n",
    "\n",
    "Let's get up and running with Snowpark Connect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1b38ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize Spark Session with Snowflake\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SnowparkConnect-Guide\") \\\n",
    "    .config(\"spark.sql.catalog.snowflake\", \"com.snowflake.snowpark.SnowflakeCatalog\") \\\n",
    "    .config(\"sfURL\", \"your-account.snowflakecomputing.com\") \\\n",
    "    .config(\"sfUser\", \"your_username\") \\\n",
    "    .config(\"sfPassword\", \"your_password\") \\\n",
    "    .config(\"sfDatabase\", \"MY_DATABASE\") \\\n",
    "    .config(\"sfSchema\", \"MY_SCHEMA\") \\\n",
    "    .config(\"sfWarehouse\", \"MY_WAREHOUSE\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark session initialized successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c855eb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Run Your First Query\n",
    "# Read a table from Snowflake\n",
    "df = spark.table(\"MY_TABLE\")\n",
    "\n",
    "# Standard PySpark operations\n",
    "result = df.filter(col(\"status\") == \"active\") \\\n",
    "    .groupBy(\"category\") \\\n",
    "    .agg(count(\"*\").alias(\"count\"), \n",
    "         sum(\"amount\").alias(\"total_amount\")) \\\n",
    "    .orderBy(desc(\"total_amount\"))\n",
    "\n",
    "# Display results\n",
    "result.show()\n",
    "\n",
    "print(\"‚úÖ First query executed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac589d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Verify Execution with Explain\n",
    "# Use explain() to see how your query is executed\n",
    "result.explain(mode=\"extended\")\n",
    "\n",
    "# This shows you:\n",
    "# - Logical plan: What you want to compute\n",
    "# - Optimized plan: How Snowflake will execute it\n",
    "# - Physical plan: The actual execution strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eda279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== READING DATA =====\n",
    "\n",
    "# 1. Read from Snowflake Tables\n",
    "# Simple table reference\n",
    "df1 = spark.table(\"MY_TABLE\")\n",
    "\n",
    "# Fully qualified name (database.schema.table)\n",
    "df2 = spark.table(\"MY_DB.MY_SCHEMA.MY_TABLE\")\n",
    "\n",
    "# Using SQL\n",
    "df3 = spark.sql(\"SELECT * FROM MY_TABLE WHERE status = 'active'\")\n",
    "\n",
    "print(\"‚úÖ Read from Snowflake tables\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c207dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Read from Snowflake Stages\n",
    "\n",
    "# Internal stage - Parquet\n",
    "df_parquet = spark.read.parquet(\"@my_stage/path/to/data/\")\n",
    "\n",
    "# External stage - CSV\n",
    "df_csv = spark.read.csv(\"@external_stage/file.csv\")\n",
    "\n",
    "# S3 via external stage\n",
    "df_s3 = spark.read.parquet(\"@s3_stage/prefix/\")\n",
    "\n",
    "# JSON with options\n",
    "df_json = spark.read \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .json(\"@my_stage/data.json\")\n",
    "\n",
    "print(\"‚úÖ Read from Snowflake stages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbfcfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== WRITING DATA =====\n",
    "\n",
    "# 1. Write to Tables\n",
    "\n",
    "# Overwrite existing table\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"MY_OUTPUT_TABLE\")\n",
    "\n",
    "# Append to existing table\n",
    "df.write.mode(\"append\").saveAsTable(\"MY_OUTPUT_TABLE\")\n",
    "\n",
    "# Create if not exists (skip if exists)\n",
    "df.write.mode(\"ignore\").saveAsTable(\"MY_OUTPUT_TABLE\")\n",
    "\n",
    "# Error if table exists (default)\n",
    "df.write.saveAsTable(\"MY_NEW_TABLE\")\n",
    "\n",
    "print(\"‚úÖ Write to Snowflake tables\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1aa109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Write to Stages\n",
    "\n",
    "# Write Parquet to internal stage\n",
    "df.write.parquet(\"@my_stage/output/\")\n",
    "\n",
    "# Write CSV with options\n",
    "df.write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"compression\", \"gzip\") \\\n",
    "    .csv(\"@my_stage/output/data.csv\")\n",
    "\n",
    "# Write with partitioning\n",
    "df.write \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .parquet(\"@my_stage/partitioned_data/\")\n",
    "\n",
    "print(\"‚úÖ Write to Snowflake stages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e756e887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of Fully Supported Operations\n",
    "\n",
    "# 1. Select, filter, aggregation\n",
    "result = spark.table(\"ORDERS\") \\\n",
    "    .select(\"order_id\", \"customer_id\", \"amount\", \"order_date\") \\\n",
    "    .filter(col(\"amount\") > 100) \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        sum(\"amount\").alias(\"total_spent\"),\n",
    "        avg(\"amount\").alias(\"avg_order_value\")\n",
    "    )\n",
    "\n",
    "# 2. Joins\n",
    "customers = spark.table(\"CUSTOMERS\")\n",
    "orders = spark.table(\"ORDERS\")\n",
    "\n",
    "joined = customers.join(orders, \"customer_id\") \\\n",
    "    .select(\n",
    "        customers[\"name\"],\n",
    "        orders[\"order_id\"],\n",
    "        orders[\"amount\"]\n",
    "    )\n",
    "\n",
    "# 3. Window functions\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n",
    "\n",
    "ranked_orders = orders \\\n",
    "    .withColumn(\"order_rank\", row_number().over(window_spec)) \\\n",
    "    .withColumn(\"running_total\", sum(\"amount\").over(window_spec))\n",
    "\n",
    "print(\"‚úÖ All fully supported operations work efficiently!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1914d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: Time-series Analysis\n",
    "orders = spark.table(\"ORDERS\")\n",
    "\n",
    "# Analyze sales trends over time\n",
    "daily_sales = orders \\\n",
    "    .withColumn(\"order_date\", to_date(col(\"order_timestamp\"))) \\\n",
    "    .groupBy(\"order_date\") \\\n",
    "    .agg(\n",
    "        count(\"order_id\").alias(\"total_orders\"),\n",
    "        sum(\"amount\").alias(\"daily_revenue\"),\n",
    "        avg(\"amount\").alias(\"avg_order_value\"),\n",
    "        max(\"amount\").alias(\"max_order\"),\n",
    "        min(\"amount\").alias(\"min_order\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"order_date\"))\n",
    "\n",
    "print(\"‚úÖ Time-series analysis example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da6d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Menu Item Performance\n",
    "# TODO - update with actual APIs and string literals\n",
    "\n",
    "menu_performance = order_detail \\\n",
    "    .join(order_header, \"ORDER_ID\") \\\n",
    "    .join(menu, \"MENU_ITEM_ID\") \\\n",
    "    .groupBy(\"MENU_ITEM_NAME\", \"ITEM_CATEGORY\") \\\n",
    "    .agg(\n",
    "        count(\"ORDER_DETAIL_ID\").alias(\"times_ordered\"),\n",
    "        sum(\"QUANTITY\").alias(\"total_quantity\"),\n",
    "        sum(\"PRICE\").alias(\"total_revenue\"),\n",
    "        avg(\"PRICE\").alias(\"avg_price\")\n",
    "    ) \\\n",
    "    .withColumn(\"revenue_per_order\", col(\"total_revenue\") / col(\"times_ordered\")) \\\n",
    "    .orderBy(desc(\"total_revenue\"))\n",
    "\n",
    "print(\"üçî Top Menu Items by Revenue:\")\n",
    "menu_performance.show(15)\n",
    "\n",
    "# Time-based analysis with date functions\n",
    "seasonal_trends = order_header \\\n",
    "    .withColumn(\"month\", month(col(\"ORDER_TS\"))) \\\n",
    "    .withColumn(\"day_of_week\", dayofweek(col(\"ORDER_TS\"))) \\\n",
    "    .withColumn(\"hour\", hour(col(\"ORDER_TS\"))) \\\n",
    "    .groupBy(\"month\", \"day_of_week\") \\\n",
    "    .agg(\n",
    "        count(\"ORDER_ID\").alias(\"order_count\"),\n",
    "        avg(\"ORDER_AMOUNT\").alias(\"avg_amount\")\n",
    "    ) \\\n",
    "    .orderBy(\"month\", \"day_of_week\")\n",
    "\n",
    "print(\"üìà Seasonal Ordering Patterns:\")\n",
    "seasonal_trends.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13270c4e",
   "metadata": {},
   "source": [
    "## Performance Considerations\n",
    "\n",
    "Follow these best practices to get optimal performance from Snowpark Connect.\n",
    "\n",
    "### 1. Use SQL Functions Over UDFs\n",
    "Python UDFs require data to be transferred to the client, processed, and sent back - this is 10-100x slower than native operations!\n",
    "\n",
    "### 2. Broadcast Joins for Small Tables\n",
    "When joining a large table with a small dimension table, use `broadcast()` to optimize the join.\n",
    "\n",
    "### 3. Cache Frequently Accessed DataFrames\n",
    "Caching creates temporary tables in Snowflake for faster repeated access. Remember to `unpersist()` when done!\n",
    "\n",
    "### 4. Minimize Data Movement\n",
    "Process data in Snowflake and only transfer final results. Avoid `collect()` on large datasets!\n",
    "\n",
    "### 5. Partition Awareness\n",
    "Filter on partitioned columns to enable partition pruning and reduce data scanned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191a359b",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "**Official Documentation:**\n",
    "- [Snowflake Documentation](https://docs.snowflake.com/)\n",
    "- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)\n",
    "\n",
    "**Next Steps:**\n",
    "- Experiment with your own data\n",
    "- Explore advanced window functions\n",
    "- Try complex multi-table joins\n",
    "- Optimize queries with explain plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9ea376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup and session management\n",
    "# When you're done, clean up cached data\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "print(\"‚úÖ Guide complete! You're ready to use Snowpark Connect for Apache Spark!\")\n",
    "print(\"üéØ Remember: Use SQL functions, cache wisely, and process in Snowflake!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
